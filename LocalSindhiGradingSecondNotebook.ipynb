{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZ3NjGpIRA03XFZVl2+YHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raheelsiddiqi2013/fruit-grading/blob/main/LocalSindhiGradingSecondNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grading of Local Sindhi Guava\n",
        "This notebook is written and executed by **Raheel Siddiqi, PhD**. Fruit grading of **Local Sindhi** guava is performed. It is a multi-class classification problem with three classes: \"Green\", \"Mature Green\", and \"Ripe\". The dataset consists of 711 images with a train-validation-test split of 70-15-15. The pre-trained VGG16 model is used for feature extraction, while a densely connected network is used for final classification. A test set classification accuracy of **98.96%** has been achieved."
      ],
      "metadata": {
        "id": "8pauHFrvqsQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Google Drive"
      ],
      "metadata": {
        "id": "E4ouipHNxOnr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1l5bIwuCjPH",
        "outputId": "dbfe5c78-88c9-4668-a246-4749af7f127d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/MyResearch/FruitGrading/LocalSindhi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUK_bHZ9DnbW",
        "outputId": "6a766778-389e-470c-85e6-35df6b9d72ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test  Train  Validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Relevant Libraries"
      ],
      "metadata": {
        "id": "SX9WlqqizuTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications import VGG16"
      ],
      "metadata": {
        "id": "4Q2yRoKhHstB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "DvFHipzdz_sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Generator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2\n",
        "                                   )\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "# Create Data Loaders\n",
        "train_loader = train_datagen.flow_from_directory(\n",
        "    directory = \"/content/drive/MyDrive/MyResearch/FruitGrading/LocalSindhi/Train\",\n",
        "    target_size = (224, 224),\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "validation_loader= validation_datagen.flow_from_directory(\n",
        "    directory = \"/content/drive/MyDrive/MyResearch/FruitGrading/LocalSindhi/Validation\",\n",
        "    target_size = (224, 224),\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "test_loader = test_datagen.flow_from_directory(\n",
        "    directory = \"/content/drive/MyDrive/MyResearch/FruitGrading/LocalSindhi/Test\",\n",
        "    target_size = (224, 224),\n",
        "    batch_size = 32,\n",
        "    shuffle = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIrCdZO4LoXB",
        "outputId": "1422d2c5-1765-4714-a497-51828b1a1c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 498 images belonging to 3 classes.\n",
            "Found 107 images belonging to 3 classes.\n",
            "Found 106 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Classes"
      ],
      "metadata": {
        "id": "-JMZBMw50Voq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7T77qcCcihP",
        "outputId": "b14d78a4-929f-4151-f20b-5fceab95b138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Green': 0, 'Mature Green': 1, 'Ripe': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Settings and Model Compilation"
      ],
      "metadata": {
        "id": "ksSv0HG50z0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.0001)\n",
        "model = Sequential()\n",
        "model.add(VGG16(include_top = False, pooling = \"avg\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['categorical_accuracy'])\n",
        "\n",
        "filepath=\"LocalSindhi_VGG16_fruit_image_classification_3_classes.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_loss\", save_best_only=True)\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "hN2FJvnqexKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6950f7-1c04-45e8-96a5-58e0940bb154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "mkVvSNZy1UL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "model_train_history = model.fit_generator(\n",
        "    generator = train_loader,\n",
        "    steps_per_epoch = math.ceil(498//32),\n",
        "    epochs = 100,\n",
        "    callbacks=callbacks_list,\n",
        "    validation_data = validation_loader,\n",
        "    validation_steps = math.ceil(107//32)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDBVfysSlXIO",
        "outputId": "0afac57e-04f5-4b7e-cb92-464ebe8e40f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-6e726d79eb7f>:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model_train_history = model.fit_generator(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - ETA: 0s - loss: 1.3955 - categorical_accuracy: 0.3863 "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 493s 31s/step - loss: 1.3955 - categorical_accuracy: 0.3863 - val_loss: 1.0653 - val_categorical_accuracy: 0.3750\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 1.0212 - categorical_accuracy: 0.4764 - val_loss: 1.0557 - val_categorical_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 15s 954ms/step - loss: 1.0791 - categorical_accuracy: 0.4485 - val_loss: 0.9717 - val_categorical_accuracy: 0.5208\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 15s 950ms/step - loss: 0.9820 - categorical_accuracy: 0.5150 - val_loss: 0.9446 - val_categorical_accuracy: 0.5104\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.9344 - categorical_accuracy: 0.4721 - val_loss: 0.9104 - val_categorical_accuracy: 0.4896\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 15s 943ms/step - loss: 0.8873 - categorical_accuracy: 0.5365 - val_loss: 0.8478 - val_categorical_accuracy: 0.5312\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.7363 - categorical_accuracy: 0.6481 - val_loss: 0.5335 - val_categorical_accuracy: 0.7708\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 14s 959ms/step - loss: 0.4326 - categorical_accuracy: 0.8176 - val_loss: 0.3369 - val_categorical_accuracy: 0.8125\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 14s 926ms/step - loss: 0.4259 - categorical_accuracy: 0.8155 - val_loss: 0.7919 - val_categorical_accuracy: 0.6250\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 14s 964ms/step - loss: 0.4132 - categorical_accuracy: 0.8069 - val_loss: 0.2227 - val_categorical_accuracy: 0.9583\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 14s 954ms/step - loss: 0.2688 - categorical_accuracy: 0.9056 - val_loss: 0.2012 - val_categorical_accuracy: 0.9271\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 14s 937ms/step - loss: 0.3579 - categorical_accuracy: 0.8541 - val_loss: 0.2261 - val_categorical_accuracy: 0.9271\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 14s 945ms/step - loss: 0.2666 - categorical_accuracy: 0.8948 - val_loss: 0.3911 - val_categorical_accuracy: 0.7292\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.4280 - categorical_accuracy: 0.8000 - val_loss: 0.4428 - val_categorical_accuracy: 0.7396\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 14s 941ms/step - loss: 0.3705 - categorical_accuracy: 0.8604 - val_loss: 0.2648 - val_categorical_accuracy: 0.8958\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.2220 - categorical_accuracy: 0.9249 - val_loss: 0.1327 - val_categorical_accuracy: 0.9479\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 14s 914ms/step - loss: 0.1818 - categorical_accuracy: 0.9270 - val_loss: 0.1844 - val_categorical_accuracy: 0.9375\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 14s 963ms/step - loss: 0.1714 - categorical_accuracy: 0.9399 - val_loss: 0.1220 - val_categorical_accuracy: 0.9583\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 15s 955ms/step - loss: 0.1835 - categorical_accuracy: 0.9333 - val_loss: 0.1393 - val_categorical_accuracy: 0.9479\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 14s 933ms/step - loss: 0.2446 - categorical_accuracy: 0.9077 - val_loss: 0.1325 - val_categorical_accuracy: 0.9479\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 14s 934ms/step - loss: 0.1883 - categorical_accuracy: 0.9335 - val_loss: 0.1434 - val_categorical_accuracy: 0.9479\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.2491 - categorical_accuracy: 0.9034 - val_loss: 0.5846 - val_categorical_accuracy: 0.7708\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 14s 916ms/step - loss: 0.4111 - categorical_accuracy: 0.8348 - val_loss: 0.2703 - val_categorical_accuracy: 0.8750\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 14s 914ms/step - loss: 0.2028 - categorical_accuracy: 0.9120 - val_loss: 0.1371 - val_categorical_accuracy: 0.9479\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 15s 974ms/step - loss: 0.1574 - categorical_accuracy: 0.9356 - val_loss: 0.1088 - val_categorical_accuracy: 0.9479\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.2937 - categorical_accuracy: 0.8863 - val_loss: 0.1226 - val_categorical_accuracy: 0.9375\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.1929 - categorical_accuracy: 0.9185 - val_loss: 0.1450 - val_categorical_accuracy: 0.9271\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 14s 918ms/step - loss: 0.1748 - categorical_accuracy: 0.9292 - val_loss: 0.1246 - val_categorical_accuracy: 0.9479\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.1336 - categorical_accuracy: 0.9485 - val_loss: 0.0905 - val_categorical_accuracy: 0.9688\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 14s 900ms/step - loss: 0.1722 - categorical_accuracy: 0.9270 - val_loss: 0.1480 - val_categorical_accuracy: 0.9583\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 14s 913ms/step - loss: 0.1730 - categorical_accuracy: 0.9313 - val_loss: 0.1654 - val_categorical_accuracy: 0.9271\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1556 - categorical_accuracy: 0.9506 - val_loss: 0.1682 - val_categorical_accuracy: 0.9271\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1760 - categorical_accuracy: 0.9142 - val_loss: 0.0984 - val_categorical_accuracy: 0.9479\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 14s 921ms/step - loss: 0.1891 - categorical_accuracy: 0.9270 - val_loss: 0.0980 - val_categorical_accuracy: 0.9479\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 14s 925ms/step - loss: 0.1650 - categorical_accuracy: 0.9313 - val_loss: 0.1293 - val_categorical_accuracy: 0.9479\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 15s 917ms/step - loss: 0.1539 - categorical_accuracy: 0.9313 - val_loss: 0.1361 - val_categorical_accuracy: 0.9479\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 14s 944ms/step - loss: 0.2427 - categorical_accuracy: 0.9077 - val_loss: 0.0925 - val_categorical_accuracy: 0.9583\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 14s 935ms/step - loss: 0.2591 - categorical_accuracy: 0.9013 - val_loss: 0.1821 - val_categorical_accuracy: 0.9167\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1804 - categorical_accuracy: 0.9249 - val_loss: 0.1097 - val_categorical_accuracy: 0.9479\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1715 - categorical_accuracy: 0.9399 - val_loss: 0.1463 - val_categorical_accuracy: 0.9479\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 14s 922ms/step - loss: 0.2053 - categorical_accuracy: 0.9313 - val_loss: 0.1559 - val_categorical_accuracy: 0.9479\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 14s 936ms/step - loss: 0.3114 - categorical_accuracy: 0.8670 - val_loss: 0.3778 - val_categorical_accuracy: 0.8125\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 15s 981ms/step - loss: 0.2686 - categorical_accuracy: 0.8691 - val_loss: 0.1935 - val_categorical_accuracy: 0.9271\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1931 - categorical_accuracy: 0.9313 - val_loss: 0.1145 - val_categorical_accuracy: 0.9583\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.1732 - categorical_accuracy: 0.9399 - val_loss: 0.1675 - val_categorical_accuracy: 0.9583\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 13s 888ms/step - loss: 0.1622 - categorical_accuracy: 0.9464 - val_loss: 0.4046 - val_categorical_accuracy: 0.8438\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 14s 911ms/step - loss: 0.1694 - categorical_accuracy: 0.9270 - val_loss: 0.1306 - val_categorical_accuracy: 0.9479\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 14s 919ms/step - loss: 0.1638 - categorical_accuracy: 0.9335 - val_loss: 0.1313 - val_categorical_accuracy: 0.9479\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 13s 882ms/step - loss: 0.1674 - categorical_accuracy: 0.9335 - val_loss: 0.2347 - val_categorical_accuracy: 0.8854\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 13s 827ms/step - loss: 0.1976 - categorical_accuracy: 0.9206 - val_loss: 0.1664 - val_categorical_accuracy: 0.9271\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 13s 843ms/step - loss: 0.1791 - categorical_accuracy: 0.9442 - val_loss: 0.3556 - val_categorical_accuracy: 0.8438\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 14s 907ms/step - loss: 0.2059 - categorical_accuracy: 0.9163 - val_loss: 0.1193 - val_categorical_accuracy: 0.9583\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 14s 906ms/step - loss: 0.1688 - categorical_accuracy: 0.9335 - val_loss: 0.0963 - val_categorical_accuracy: 0.9479\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 15s 996ms/step - loss: 0.1362 - categorical_accuracy: 0.9528 - val_loss: 0.1168 - val_categorical_accuracy: 0.9583\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 17s 1s/step - loss: 0.1408 - categorical_accuracy: 0.9442 - val_loss: 0.1191 - val_categorical_accuracy: 0.9271\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 14s 905ms/step - loss: 0.1403 - categorical_accuracy: 0.9399 - val_loss: 0.1072 - val_categorical_accuracy: 0.9583\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 14s 916ms/step - loss: 0.1343 - categorical_accuracy: 0.9549 - val_loss: 0.0952 - val_categorical_accuracy: 0.9583\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.1254 - categorical_accuracy: 0.9399 - val_loss: 0.0767 - val_categorical_accuracy: 0.9792\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 14s 933ms/step - loss: 0.1544 - categorical_accuracy: 0.9292 - val_loss: 0.1051 - val_categorical_accuracy: 0.9479\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 14s 893ms/step - loss: 0.1871 - categorical_accuracy: 0.9249 - val_loss: 0.3334 - val_categorical_accuracy: 0.8021\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 14s 917ms/step - loss: 0.2477 - categorical_accuracy: 0.8927 - val_loss: 0.1167 - val_categorical_accuracy: 0.9479\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 14s 906ms/step - loss: 0.2308 - categorical_accuracy: 0.9056 - val_loss: 0.2077 - val_categorical_accuracy: 0.9167\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 14s 940ms/step - loss: 0.1520 - categorical_accuracy: 0.9421 - val_loss: 0.0789 - val_categorical_accuracy: 0.9792\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 14s 892ms/step - loss: 0.1710 - categorical_accuracy: 0.9185 - val_loss: 0.1282 - val_categorical_accuracy: 0.9375\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 15s 959ms/step - loss: 0.1396 - categorical_accuracy: 0.9506 - val_loss: 0.0877 - val_categorical_accuracy: 0.9688\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 14s 900ms/step - loss: 0.2454 - categorical_accuracy: 0.9120 - val_loss: 0.2324 - val_categorical_accuracy: 0.8958\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 16s 1s/step - loss: 0.1650 - categorical_accuracy: 0.9399 - val_loss: 0.1143 - val_categorical_accuracy: 0.9479\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 13s 852ms/step - loss: 0.1537 - categorical_accuracy: 0.9399 - val_loss: 0.1068 - val_categorical_accuracy: 0.9375\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 14s 936ms/step - loss: 0.1459 - categorical_accuracy: 0.9485 - val_loss: 0.0720 - val_categorical_accuracy: 0.9583\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 14s 948ms/step - loss: 0.1388 - categorical_accuracy: 0.9464 - val_loss: 0.0675 - val_categorical_accuracy: 0.9688\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 14s 910ms/step - loss: 0.1235 - categorical_accuracy: 0.9542 - val_loss: 0.1014 - val_categorical_accuracy: 0.9479\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 14s 899ms/step - loss: 0.1454 - categorical_accuracy: 0.9549 - val_loss: 0.2054 - val_categorical_accuracy: 0.9167\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 14s 904ms/step - loss: 0.1610 - categorical_accuracy: 0.9485 - val_loss: 0.1347 - val_categorical_accuracy: 0.9583\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 15s 986ms/step - loss: 0.1714 - categorical_accuracy: 0.9356 - val_loss: 0.0997 - val_categorical_accuracy: 0.9688\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 13s 889ms/step - loss: 0.1573 - categorical_accuracy: 0.9506 - val_loss: 0.1935 - val_categorical_accuracy: 0.9167\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 13s 892ms/step - loss: 0.2296 - categorical_accuracy: 0.9034 - val_loss: 0.0958 - val_categorical_accuracy: 0.9792\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 14s 909ms/step - loss: 0.1266 - categorical_accuracy: 0.9506 - val_loss: 0.1302 - val_categorical_accuracy: 0.9583\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 14s 898ms/step - loss: 0.1748 - categorical_accuracy: 0.9167 - val_loss: 0.1329 - val_categorical_accuracy: 0.9479\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 14s 895ms/step - loss: 0.1565 - categorical_accuracy: 0.9335 - val_loss: 0.1919 - val_categorical_accuracy: 0.9271\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 14s 903ms/step - loss: 0.2631 - categorical_accuracy: 0.8948 - val_loss: 0.2603 - val_categorical_accuracy: 0.8646\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 15s 1s/step - loss: 0.2778 - categorical_accuracy: 0.8884 - val_loss: 0.1688 - val_categorical_accuracy: 0.9375\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 13s 839ms/step - loss: 0.1523 - categorical_accuracy: 0.9464 - val_loss: 0.1639 - val_categorical_accuracy: 0.9271\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 14s 914ms/step - loss: 0.1547 - categorical_accuracy: 0.9506 - val_loss: 0.1320 - val_categorical_accuracy: 0.9479\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 15s 996ms/step - loss: 0.1601 - categorical_accuracy: 0.9335 - val_loss: 0.1565 - val_categorical_accuracy: 0.9375\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 14s 904ms/step - loss: 0.1306 - categorical_accuracy: 0.9421 - val_loss: 0.0897 - val_categorical_accuracy: 0.9479\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 13s 892ms/step - loss: 0.1481 - categorical_accuracy: 0.9421 - val_loss: 0.1154 - val_categorical_accuracy: 0.9583\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 14s 903ms/step - loss: 0.1167 - categorical_accuracy: 0.9485 - val_loss: 0.0989 - val_categorical_accuracy: 0.9688\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 14s 905ms/step - loss: 0.1084 - categorical_accuracy: 0.9592 - val_loss: 0.0808 - val_categorical_accuracy: 0.9688\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 14s 888ms/step - loss: 0.1523 - categorical_accuracy: 0.9506 - val_loss: 0.1073 - val_categorical_accuracy: 0.9375\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 14s 918ms/step - loss: 0.1343 - categorical_accuracy: 0.9442 - val_loss: 0.0621 - val_categorical_accuracy: 0.9792\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 14s 914ms/step - loss: 0.1160 - categorical_accuracy: 0.9549 - val_loss: 0.2206 - val_categorical_accuracy: 0.9167\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 13s 890ms/step - loss: 0.1450 - categorical_accuracy: 0.9399 - val_loss: 0.1410 - val_categorical_accuracy: 0.9375\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 14s 910ms/step - loss: 0.1209 - categorical_accuracy: 0.9549 - val_loss: 0.0832 - val_categorical_accuracy: 0.9583\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 17s 1s/step - loss: 0.1188 - categorical_accuracy: 0.9571 - val_loss: 0.1400 - val_categorical_accuracy: 0.9583\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 14s 905ms/step - loss: 0.1054 - categorical_accuracy: 0.9721 - val_loss: 0.1004 - val_categorical_accuracy: 0.9583\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 14s 896ms/step - loss: 0.1536 - categorical_accuracy: 0.9335 - val_loss: 0.1253 - val_categorical_accuracy: 0.9583\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 15s 991ms/step - loss: 0.1486 - categorical_accuracy: 0.9399 - val_loss: 0.1082 - val_categorical_accuracy: 0.9479\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 13s 889ms/step - loss: 0.1018 - categorical_accuracy: 0.9592 - val_loss: 0.0989 - val_categorical_accuracy: 0.9583\n",
            "Epoch 99/100\n",
            " 6/15 [===========>..................] - ETA: 6s - loss: 0.0745 - categorical_accuracy: 0.9688"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Testing"
      ],
      "metadata": {
        "id": "oT44TLf52QIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "best_model = load_model('LocalSindhi_VGG16_fruit_image_classification_3_classes.h5')\n",
        "steps_test=int(106/32)\n",
        "result = best_model.evaluate_generator(test_loader, steps=steps_test,verbose=1)\n",
        "print(\"Test-set accuracy: {0:.2%}\".format(result[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r__7vm_JUtSt",
        "outputId": "1abbe705-0559-47de-ab97-06e3765466db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-de7246e46616>:4: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  result = best_model.evaluate_generator(test_loader, steps=steps_test,verbose=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 1s 309ms/step - loss: 0.0867 - categorical_accuracy: 0.9896\n",
            "Test-set accuracy: 98.96%\n"
          ]
        }
      ]
    }
  ]
}